{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea7b224",
   "metadata": {},
   "source": [
    "# ITCS 4152/5152 Assignment3\n",
    "**Due date: 11:59 pm EST on Feb 27, 2023 (Fri.)**\n",
    "\n",
    "---\n",
    "In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your UNC Charlotte (*.uncc.edu) account for coding and Google Drive to save your results.\n",
    "\n",
    "\n",
    "## Google Colab Tutorial\n",
    "---\n",
    "Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n",
    "\n",
    "Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n",
    "\n",
    "\n",
    "## Using SIFT in OpenCV 3.x.x in Colab\n",
    "---\n",
    "The default version of OpenCV in Colab is 3.4.3. If we use SIFT method directly, typically we will get this error message:\n",
    "\n",
    "```\n",
    "error: OpenCV(3.4.3) /io/opencv_contrib/modules/xfeatures2d/src/sift.cpp:1207: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'create'\n",
    "\n",
    "```\n",
    "\n",
    "One simple way to use the OpenCV in-built function `SIFT` in Colab is to switch the version to the one from 'contrib'. Below is an example of switching OpenCV version:\n",
    "\n",
    "1. Run the following command in one section in Colab, which has already been included in this assignment:\n",
    "```\n",
    "pip install opencv-contrib-python\n",
    "```\n",
    "2. Restart runtime by\n",
    "```\n",
    "Runtime -> Restart Runtime\n",
    "```\n",
    "\n",
    "Then you should be able to use use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n",
    "\n",
    "## Some Resources\n",
    "---\n",
    "In addition to the tutorial document, the following resources can definitely help you in this homework:\n",
    "- http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n",
    "- http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html\n",
    "- http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html?highlight=sift#cv2.SIFT\n",
    "- http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages here\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import itertools\n",
    "import time\n",
    "import zipfile\n",
    "import torch\n",
    "import torchvision\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "print(cv2.__version__) # verify OpenCV version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb723fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount your google drive where you've saved your assignment folder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989050de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '------' with the path such that \"ITCS_4152_5152_assignment2\" is your working directory\n",
    "cd '/content/gdrive/My Drive/------'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca9ee9",
   "metadata": {},
   "source": [
    "## Problem 1: Image Classification with Bag of SIFT Representation + SVM Classifer\n",
    "\n",
    "## Description\n",
    "---\n",
    "In this problem, we will examine the task of image classification using bags of quantized local features and linear classifiers learned by support vector machines. We will implement a basic bag of words model and\n",
    "classify images into one of 10 categories by training and testing on **a small subset** of **miniImagenet** dataset (downloaded from https://awesomeopensource.com/project/oscarknagg/few-shot).\n",
    "\n",
    "Bag of words models are a popular technique for image classification inspired by\n",
    "models used in natural language processing. The model ignores or downplays word\n",
    "arrangement (spatial information in the image) and classifies based on a\n",
    "histogram of the frequency of visual words. The visual word \"vocabulary\" is\n",
    "established by clustering a large corpus of local features. See Szeliski chapter\n",
    "14.4.1 for more details on category recognition with quantized features. In\n",
    "addition, 14.3.2 discusses vocabulary creation and 14.1 covers classification\n",
    "techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ed518",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7470b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict_train = pickle.load(fo, encoding='bytes')\n",
    "        dict_test = pickle.load(fo, encoding='bytes')\n",
    "    return dict_train, dict_test\n",
    "\n",
    "# load data from pickle files and sample a small subset for this homework\n",
    "train_data_dict, test_data_dict = unpickle('miniImageNet.pkl') \n",
    "train_label = train_data_dict['labels']  \n",
    "train_data = train_data_dict['data']  \n",
    "\n",
    "test_label = test_data_dict['labels']   \n",
    "test_data = test_data_dict['data'] \n",
    "\n",
    "# reshape data\n",
    "train_data = [d.reshape(3, 128, 128).transpose(1,2,0) for d in train_data]\n",
    "test_data = [d.reshape(3, 128, 128).transpose(1,2,0) for d in test_data]\n",
    "\n",
    "# plot 10 training images from 10 classes\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.title('Sample Image of Class {}'.format(i))\n",
    "    plt.imshow(cv2.cvtColor(train_data[500*i], cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.title('Sample Image of Class {}'.format(i+5))\n",
    "    plt.imshow(cv2.cvtColor(train_data[500*(i+5)], cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecb8a2",
   "metadata": {},
   "source": [
    "## Problem 1.a: Bag of SIFT Representation \n",
    "{50 points}\n",
    "Before we can represent our training and testing images as bag of\n",
    "feature histograms, we first need to establish a vocabulary of visual words. We\n",
    "will form this vocabulary by sampling many local features from our training set\n",
    "(10's or 100's of thousands) and then cluster them with k-means. The number of\n",
    "k-means clusters is the size of our vocabulary and the size of our features. For\n",
    "example, you might start by clustering many SIFT descriptors into k=50 clusters.\n",
    "This partitions the continuous, 128 dimensional SIFT feature space into 50\n",
    "regions. For any new SIFT feature we observe, we can figure out which region it\n",
    "belongs to as long as we save the centroids of our original clusters. Those\n",
    "centroids are our visual word vocabulary. \n",
    "\n",
    "Now we are ready to represent our training and testing images as histograms of\n",
    "visual words. For each image we will densely sample many SIFT descriptors.\n",
    "Instead of storing hundreds of SIFT descriptors, we simply count how many SIFT\n",
    "descriptors fall into each cluster in our visual word vocabulary. This is done\n",
    "by finding the nearest neighbor k-means centroid for every SIFT feature. Thus,\n",
    "if we have a vocabulary of 50 visual words, and we detect 200 distinct SIFT\n",
    "features in an image, our bag of SIFT representation will be a histogram of 50\n",
    "dimensions where each bin counts how many times a SIFT descriptor was assigned\n",
    "to that cluster. The total of all the bin-counts is 200. The histogram should be\n",
    "normalized so that image size does not dramatically change the bag of features\n",
    "magnitude.\n",
    "\n",
    "\n",
    "**Note**: \n",
    "- Instead of using SIFT to detect invariant keypoints which is time-consuming,\n",
    "  you are recommended to **densely sample keypoints** in a grid with certain step\n",
    "  size (sampling density) and scale. \n",
    "- There are many design decisions and free parameters for the bag of SIFT\n",
    "  representation (number of clusters, sampling density, sampling scales, SIFT\n",
    "  parameters, etc.) so accuracy might vary.\n",
    "\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "- Use [KMeans in Sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "  to do clustering and find the nearest cluster centroid for each SIFT feature;\n",
    "- Use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object;\n",
    "- Use [`cv2.Keypoint()`](https://docs.opencv.org/3.0-beta/modules/core/doc/basic_structures.html?highlight=keypoint#keypoint) to generate key points;\n",
    "- Use `sift.compute()` to compute SIFT descriptors given densely sampled keypoints.\n",
    "\n",
    "- Be mindful of RAM usage. Try to make the code more memory efficient, otherwise it could easily exceed RAM limits in Colab, at which point your session will crash.\n",
    "- If your RAM is going to run out of space, use [gc.collect()](https://docs.python.org/3/library/gc.html) for the garbage collector to collect unused objects in  memory to free some space.\n",
    "- Store data or features as NumPy arrays instead of lists. Computation on NumPy arrays is much more efficient than lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31648ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "np.random.seed(65)\n",
    "\n",
    "##########--WRITE YOUR CODE HERE--##########\n",
    "# The following steps are just for your reference\n",
    "# You can write in your own way\n",
    "#\n",
    "# # densely sample keypoints\n",
    "# def sample_kp(shape, stride, size):\n",
    "#     return kp\n",
    "# \n",
    "# # extract vocabulary of SIFT features\n",
    "# def extract_vocabulary(raw_data, key_point):\n",
    "#     return vocabulary   \n",
    "# \n",
    "# # extract Bag of SIFT Representation of images\n",
    "# def extract_feat(raw_data, vocabulary, key_point): \n",
    "#     return feat\n",
    "# \n",
    "# # sample dense keypoints \n",
    "# skp = sample_kp((train_data[0].shape[0],train_data[0].shape[1]),(64,64), 8)\n",
    "# vocabulary = extract_vocabulary(train_data, skp)  \n",
    "# train_feat = extract_feat(train_data, vocabulary, skp)\n",
    "# test_feat = extract_feat(test_data, vocabulary, skp)\n",
    "\n",
    "train_feat =\n",
    "test_feat = \n",
    "\n",
    "##########-------END OF CODE-------##########\n",
    "# this block should generate \n",
    "# train_feat and test_feat corresponding to train_data and test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade3f31",
   "metadata": {},
   "source": [
    "## Problem 1.b: one-vs-all SVMs\n",
    "{50 points}\n",
    "You do not have to implement the support vector machine. However, linear\n",
    "classifiers are inherently binary and we have a 10-way classification problem\n",
    "(the library has handled it for you). To decide which of 10 categories a test\n",
    "case belongs to, you will train 10 binary, one-vs-all SVMs. One-vs-all means\n",
    "that each classifier will be trained to recognize 'bird' vs 'non-bird',\n",
    "'cat' vs 'non-cat', etc. All 10 classifiers will be evaluated on each\n",
    "test case and the classifier which is most confidently positive \"wins\". E.g. if\n",
    "the 'cat' classifier returns a score of -0.2 (where 0 is on the decision\n",
    "boundary), and the 'bird' classifier returns a score of -0.3, and all of the\n",
    "other classifiers are even more negative, the test case would be classified as a\n",
    "'cat' even though none of the classifiers put the test case on the positive\n",
    "side of the decision boundary. When learning an SVM, you have a free parameter\n",
    "**C** which controls how strongly regularized the model is. Your\n",
    "accuracy will be very sensitive to **C**, so be sure to try many values.\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "- Use SVM in\n",
    "  [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
    "  (recommended) or\n",
    "  [OpenCV](https://docs.opencv.org/3.0-alpha/modules/ml/doc/support_vector_machines.html)\n",
    "  to do training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51caa655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "##########--WRITE YOUR CODE HERE--##########\n",
    "# The following steps are just for your reference\n",
    "# You can write in your own way\n",
    "#\n",
    "# def train_SVM(X, Y):\n",
    "#     return clf\n",
    "# \n",
    "# def predict_SVM(clf, X):\n",
    "#     return predict\n",
    "#   \n",
    "# clf = train_SVM(train_feat, train_label)\n",
    "# \n",
    "# # make predictions on test data\n",
    "# prediction = [-1]*len(test_feat)\n",
    "# for i in range(len(test_feat)):\n",
    "#     prediction[i] = predict_SVM(clf, np.reshape(test_feat[i],(1,-1)))\n",
    "# \n",
    "# test_label_pred = np.reshape(np.array(predictions),(-1,))\n",
    "\n",
    "\n",
    "\n",
    "test_label_pred =\n",
    "\n",
    "##########-------END OF CODE-------##########\n",
    "accuracy = sum(np.array(test_label_pred) == test_label) / float(len(test_label))\n",
    "print(\"The accuracy of Bag of SIFT Representation + one-vs-all SVMs model is {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c72ed",
   "metadata": {},
   "source": [
    "## Submission guidelines\n",
    "---\n",
    "Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_assignment2_part1' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs. **DO NOT** zip and upload the dataset on canvas due to size limit.\n",
    "\n",
    "When submitting your .zip file through blackboard, please\n",
    "-- name your .zip file as **Surname_Givenname_assignment2_part1.zip**.\n",
    "\n",
    "This zip file should include:\n",
    "```\n",
    "Surname_Givenname_UNCCID_assignment2_part1  \n",
    "        |---Surname_Givenname_UNCCID_assignment2_part1.ipynb\n",
    "        |---Surname_Givenname_UNCCID_assignment2_part1.pdf\n",
    "```\n",
    "\n",
    "For instance, student Yann Lecun should submit a zip file named \"Lecun_Yann_111134567_assignment2_part1.zip\" for assignment2_part1 in this structure:\n",
    "```\n",
    "Lecun_Yann_111134567_assignment2_part1\n",
    "        |---Lecun_Yann_111134567_assignment2_part1.ipynb\n",
    "        |---Lecun_Yann_111134567_assignment2_part1.pdf\n",
    "```\n",
    "\n",
    "Then right click this folder, click ***Get shareable link***, in the People textfield, enter TA's emails: ***psingire@uncc.edu*** and ***kchiguru@uncc.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n",
    "\n",
    "Note that in google colab, we will only grade the version of the code right before the timestamp of the submission made in canvas. \n",
    "\n",
    "Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_UNCCID_assignment1' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs.\n",
    "\n",
    "\n",
    "-- DO NOT change the folder structure, please just fill in the blanks. <br>\n",
    "\n",
    "You are encouraged to post and answer questions on Canvas. Please ask questions on Canvas and send emails only for personal issues.\n",
    "\n",
    "If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n",
    "\n",
    "Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work.\n",
    "\n",
    "Late submission penalty: <br>\n",
    "There will be a 10% penalty per day for late submission. However, you will have THREE days throughout the whole semester to submit late without penalty. Note that the grace period is calculated by days instead of hours. If you submit the homework one minute after the deadline, one late day will be counted. Likewise, if you submit one minute after the deadline, the 10% penaly will be imposed if not using the grace period.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
